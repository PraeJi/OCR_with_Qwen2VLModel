# -*- coding: utf-8 -*-
"""OCR_image_qwen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hju8E-6ziJUy7D2WkCHNySsjZ0YFAXIj
"""

from PIL import Image
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
import io
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("model loading...")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    torch_dtype = torch.float16,
    device_map = {"": device}
)
model.eval()
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
print("model loaded")

@torch.inference_mode()
def ocr_image(img, prompt):

  # prompt = "Extract text from the image (OCR)"
  messages = [
      {
          "role": "user",
          "content": [
              {"type": "image"},
              {"type": "text", "text": prompt}
          ]
      }
  ]

  text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
  inputs = processor(
      text = [text_prompt],
      images = [img],
      padding = True,
      return_tensors = "pt"
  )

  # move to GPU
  inputs = {k: v.to(device) for k,v in inputs.items()}

  # generate
  with torch.autocast("cuda"):
    output = model.generate(**inputs, max_new_tokens=512)

  generated = output[:, inputs["input_ids"].shape[1]:]

  text = processor.batch_decode(
      generated,
      skip_special_tokens = True
  )[0]

  return text

"""# For test ocr_image function"""
prompt = "Extract text from the image (OCR)"
image_path = "/content/Screenshot 2025-11-29 212227.png"
result = ocr_image(image_path, prompt)

print(result)

image_path1 = "https://media.istockphoto.com/id/914023158/vector/clean-minimal-invoice-vector-template-design.jpg?s=612x612&w=0&k=20&c=bUdGOlc4-xIrL6qyjN1P3izkfOM3sbb5EqYYiPJCPrc="
result_url = ocr_image(image_path1, prompt)

print(result_url)

