# -*- coding: utf-8 -*-
"""OCR_and_ClassifyContent_QwenModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ItjOy57BMDx1EpFJ5qOHpch5A8b8T71T
"""

from PIL import Image
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
import io
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("model loading...")

model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    torch_dtype = torch.float16,
    device_map = {"": device}
)
model.eval()
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

print("model loaded")

@torch.inference_mode()
def ocr_and_classify(img):

  schema_info = content_requests_schema()  # JSON example + constraints

  prompt = f"""You are an OCR extraction assistant.

          Task:
          Extract text from the image (OCR)

          Output rules:
          1. Return ONLY JSON according to the schema below.
          2. Do not add any extra text, explanation, or commentary.
          3. All fields must be present. If the information cannot be found in the image, use "-".
          4. Do not change the structure or field names.
          5. Strictly follow the example structure.

          Schema Description: {schema_info['description']}

          Example JSON:
          {schema_info['example']}

          Constraints:
          {schema_info['constraints']}
      """

  messages = [
      {
          "role": "user",
          "content": [
              {"type": "image"},
              {"type": "text", "text": prompt}
          ]
      }
  ]

  text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
  inputs = processor(
      text = [text_prompt],
      images = [img],
      padding = True,
      return_tensors = "pt"
  )

  # move to GPU
  inputs = {k: v.to(device) for k,v in inputs.items()}

  # generate
  with torch.autocast("cuda"):
    output = model.generate(**inputs, max_new_tokens=1024)

  generated = output[:, inputs["input_ids"].shape[1]:]

  text = processor.batch_decode(
      generated,
      skip_special_tokens = True
  )[0]

  return text

def content_requests_schema():
    """
    คืนค่า schema สำหรับ embed ใน prompt ของ Qwen
    เป็น JSON ตัวอย่าง + constraints
    """
    schema = {
        "description": "โปรดตอบกลับเป็น JSON ตาม schema นี้เท่านั้น (ภาษาไทย)",
        "example": {
            "ชื่อ Account": "นางสาว พิมพ์พิชา ใจดี",
            "ที่อยู่":"บ้านเลขที่ 45/7 ถ.พัฒนา ซอยจงใจ ต.ในเมือง อ.เมือง จ.เชียงใหม่ รหัสไปรษณีย์ 50000",
            "ข้อมูลที่ขอความช่วยเหลือ": "ต้องการอาหารและน้ำดื่มสำหรับครอบครัว 4 คน เนื่องจากบ้านถูกน้ำท่วม",
            "ประเภทความช่วยเหลือ": "อาหาร",
            "เบอร์โทร": ["0812345678", "+66891234567"],
            "เวลาที่สะดวกติดต่อ": "09:00-18:00",
            "ฉุกเฉิน": True
        },
        "constraints": {
            "ชื่อ Account": "string, min 0 char, max 200",
            "ที่อยู่": "string",
            "ข้อมูลที่ขอความช่วยเหลือ": "string",
            "ประเภทความช่วยเหลือ": "enum: ['การแพทย์','การเงิน','ที่พักอาศัย','อาหาร','กฎหมาย/ปรึกษา','อื่นๆ']",
            "เบอร์โทร": "array of string, regex ^(?:\\+66|0)\\d{8,9}$",
            "เวลาที่สะดวกติดต่อ": "string, optional",
            "ฉุกเฉิน": "boolean"
        }
    }

    return schema

# For testing
# img = "/content/588497508_26033068199629460_2091224368113351131_n.jpg"
# result = ocr_and_classify(img)

# print(result)

"""```json
{
  "ชื่อ Account": "Phang Emwadee",
  "ที่อยู่": "บ้านเลขที่ 98 ช่อง 32 ถ.ราชพฤกษ์อุทิศ มีเด็ก 6 คนและผู้ใหญ่ 2 คนและในช่องอิ่งหลายคนข้าว",
  "ข้อมูลที่ขอความช่วยเหลือ": "ต้องการอาหารและน้ำดื่มสำหรับครอบครัว 4 คน เนื่องจากบ้านถูกน้ำท่วม",
  "ประเภทความช่วยเหลือ": "อาหาร",
  "เบอร์โทร": "0887849534 0914617995",
  "เวลาที่สะดวกติดต่อ": "09:00-18:00",
  "ฉุกเฉิน": true
}
```
"""

